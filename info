ğŸ§  1ï¸âƒ£ Why This Dataset â€” Telco Customer Churn

Dataset name: Telco Customer Churn (IBM)


ğŸ“˜ Description:

This dataset contains telecom customer data with features like:

Gender, Contract type, Tenure, InternetService, Payment method, etc.

Target variable: Churn (Yes/No) â€” whether the customer left the service.

ğŸ’¡ Why I chose it:
Reason	Explanation
ğŸ§© Real-world business relevance	Telecom churn prediction is a classic problem used to reduce customer loss.
âš™ï¸ Data imperfection	Contains missing values, string-based numbers, and categorical columns â€” perfect for demonstrating cleaning & preprocessing.
ğŸ“Š Binary classification	Ideal for applying and comparing multiple ML models.
ğŸ§  Feature richness	Many meaningful features (tenure, contract, etc.) that can be visualized and interpreted easily.

Viva answer:

â€œI selected the Telco Churn dataset because it represents a real business problem with imperfect data.
It allowed me to demonstrate data cleaning, feature engineering, model comparison, and visualization â€” all in one project.â€

ğŸ§  2ï¸âƒ£ Why These Models

We used four models, each chosen for a reason â€” so the invigilator sees logic, not random choice:

Model	Type	Why I Chose It	Strength
Logistic Regression	Linear	Simple, interpretable baseline for binary classification.	Easy to explain mathematically.
Random Forest	Ensemble Tree	Handles non-linear relationships, robust to noise.	Prevents overfitting, gives feature importance.
SVM (RBF Kernel)	Non-linear classifier	Finds best hyperplane even when data isnâ€™t linearly separable.	Works well for complex data.
Gradient Boosting	Sequential Ensemble	Builds multiple small models to fix previous errors.	High accuracy, handles mixed data types well.

Viva answer:

â€œI used a combination of linear (Logistic Regression), non-linear (SVM), and ensemble models (Random Forest, Gradient Boosting).
This allowed me to compare simple vs advanced models and select the one that balances accuracy and interpretability.â€

ğŸ§  3ï¸âƒ£ Evaluation Metrics â€” Why & What They Mean

In binary classification (like churn), accuracy alone isnâ€™t enough, so we use a mix of metrics to judge performance fairly.

Metric	Formula	What It Measures	Why I Used It
Accuracy	(TP+TN)/(TP+TN+FP+FN)	% of total correct predictions	General measure of correctness
Precision	TP / (TP + FP)	Of all predicted churns, how many were correct	Avoids false alarms (wasting retention efforts)
Recall	TP / (TP + FN)	Of all actual churners, how many did we catch	Measures ability to find churners (most important in telecom)
F1 Score	2 Ã— (Precision Ã— Recall)/(Precision + Recall)	Balance between precision & recall	Good for imbalanced data
ROC-AUC	Area under ROC curve	How well model separates churn vs non-churn	Evaluates ranking ability, not just threshold-based classification

Viva answer:

â€œAccuracy alone can be misleading, especially if most customers donâ€™t churn.
So I used F1 and ROC-AUC to balance false positives and negatives.
In telecom, recall is most critical â€” missing a churner costs money.
ROC-AUC gives an overall idea of modelâ€™s discrimination ability.â€

ğŸ§  4ï¸âƒ£ Why Gradient Boosting Was Best

After testing all models:

Logistic Regression â†’ decent, but limited to linear patterns.

SVM â†’ good, but slower and less interpretable.

Random Forest â†’ strong, but slightly less accurate here.

Gradient Boosting â†’ highest Accuracy, ROC-AUC, and F1 Score.

ğŸ” Why it performed best:
Reason	Explanation
ğŸ“ˆ Sequential Learning	It builds trees one after another, each fixing the errors of the previous one.
âš–ï¸ Optimizes bias & variance	Lower bias (like linear models) and lower variance (like trees).
ğŸ” Handles complex feature interactions	Works well with non-linear data and mixed categorical/numeric data.
ğŸ’¡ Feature weighting	Assigns importance to key features like tenure, contract type, etc.

Viva answer:

â€œGradient Boosting performed best because it learns iteratively â€” each tree corrects the previous treeâ€™s mistakes.
It balances bias and variance effectively, leading to high accuracy and AUC.
Also, it captures non-linear relationships better than Logistic Regression or SVM.â€

ğŸ§  5ï¸âƒ£ Visualizations and Why I Used Them

Visuals not only make the project attractive â€” they help explain and justify results.
Hereâ€™s what each one means and why it matters:

Visualization	What It Shows	Why I Used It
Bar Chart (Model Comparison)	Accuracy, ROC-AUC, F1 of each model	Quick visual of which model performs best.
ROC Curve	Trade-off between True Positive Rate and False Positive Rate	Compares discrimination ability visually.
Confusion Matrix	Breakdown of correct vs incorrect predictions	Identifies types of errors (false negatives vs positives).
Feature Importance (Bar Plot)	Which features influence churn most	Business interpretability â€” tells company what drives churn.
SHAP Plot	Feature impact on individual predictions	Explainable AI â€” shows â€œwhyâ€ for each prediction.
Churn Probability Distribution	Histogram of churn probabilities	Helps visualize which customers are high-risk.
Customer-wise Scatter Plot	Churn probability vs customer ID	Shows overall churn pattern across customers.

Viva answer:

â€œI used visualization to make the analysis interpretable.
For example, the bar chart compares models, ROC shows separation ability, and SHAP explains feature influence.
These visuals make the results easy to understand for non-technical stakeholders.â€

ğŸ§  6ï¸âƒ£ Bonus: How to Conclude Confidently

â€œIn summary, I selected the Telco Churn dataset for its real-world value and imperfections.
After preprocessing and feature engineering, I tested four models and used multiple evaluation metrics to compare them.
Gradient Boosting performed the best because it learns iteratively and handles complex patterns.
I used visualizations like ROC, confusion matrix, and SHAP to interpret the model both technically and business-wise.
The final model can help telecom companies identify customers likely to churn and take preventive actions.â€

ğŸ”‘ Quick Summary Cheat-Sheet (For Rapid Revision)
Topic	2-Line Explanation
Dataset Choice	Real-world, imperfect, perfect for full ML pipeline demo.
Models Used	Logistic (baseline), RF & GB (ensembles), SVM (non-linear).
Best Model (GB)	Learns sequentially, corrects errors, handles complexity.
Metrics	F1 & ROC-AUC preferred over accuracy for imbalance.
Visualizations	Used to compare models, analyze errors, and interpret results.





ğŸ§© STEP 1: IMPORT LIBRARIES
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import shap
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import (
    accuracy_score, confusion_matrix, classification_report,
    roc_auc_score, roc_curve, f1_score, precision_score, recall_score
)
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC

ğŸ” Explanation:

pandas â†’ helps to handle data in tables (called DataFrames).

numpy â†’ handles numerical operations (arrays, math, stats).

matplotlib & seaborn â†’ used to make graphs and plots.

plotly.express â†’ used for interactive, beautiful visualizations.

shap â†’ library that explains why the model predicted something (explainable AI).

sklearn â†’ Scikit-learn, the main ML library for model training, testing, and evaluation.

ğŸ‘‰ In viva:
"We import all these libraries to perform data cleaning, visualization, model training, and evaluation easily."

ğŸ§© STEP 2: LOAD DATA
df = pd.read_csv("Telco-Customer-Churn.csv")

ğŸ” Explanation:

pd.read_csv() â†’ reads a CSV file (spreadsheet) into a pandas DataFrame.

df â†’ variable holding your dataset.

ğŸ‘‰ In viva:
"I loaded the dataset using pandas. The dataset contains telecom customers with details like their plan, internet usage, and whether they churned or not."

ğŸ§© STEP 3: CLEANING & PREPROCESSING
df.columns = df.columns.str.strip()
df['TotalCharges'] = df['TotalCharges'].replace(" ", np.nan)
df['TotalCharges'] = df['TotalCharges'].astype(float)
df['TotalCharges'].fillna(df['TotalCharges'].median(), inplace=True)
df.drop('customerID', axis=1, inplace=True)

ğŸ” Explanation:

.str.strip() â†’ removes unwanted spaces from column names.

.replace(" ", np.nan) â†’ replaces blank cells with â€œNaNâ€ (missing value).

.astype(float) â†’ converts that column from text to numeric.

.fillna(..., inplace=True) â†’ fills missing values with the median value.

.drop('customerID') â†’ removes that column because itâ€™s just an ID, not useful for prediction.

ğŸ‘‰ In viva:
"I cleaned the data by removing spaces, converting text to numbers, and handling missing values using the median, which prevents bias. I dropped CustomerID as it doesnâ€™t influence churn."

ğŸ§© STEP 4: FEATURE SCALING & SPLITTING
le = LabelEncoder()
for col in df.select_dtypes('object').columns:
    df[col] = le.fit_transform(df[col])

ğŸ” Explanation:

LabelEncoder â†’ converts text (like "Male"/"Female") into numbers (1/0).

.fit_transform() â†’ learns the encoding and applies it.

ğŸ‘‰ In viva:
"Since ML models only work with numbers, I converted categorical data into numeric form using LabelEncoder."

Splitting Data:
X = df.drop('Churn', axis=1)
y = df['Churn']


X â†’ independent variables (customer features).

y â†’ dependent variable (target = Churn or Not).

Scaling and Train-Test Split:
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, stratify=y
)


StandardScaler â†’ normalizes all features (so large numbers donâ€™t dominate small ones).

Train-Test Split â†’ splits data into 80% for training, 20% for testing.

random_state=42 â†’ ensures same random split every time.

stratify=y â†’ keeps same proportion of churners in both sets.

ğŸ‘‰ In viva:
"I scaled features for uniformity and split data into training and testing to check real-world performance."

ğŸ§© STEP 5: TRAINING MODELS
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Random Forest": RandomForestClassifier(n_estimators=300, random_state=42),
    "SVM (RBF Kernel)": SVC(kernel='rbf', probability=True, random_state=42),
    "Gradient Boosting": GradientBoostingClassifier(n_estimators=150, random_state=42)
}

ğŸ” Explanation of models:
Model	Description
Logistic Regression	Best for linear relationships; interpretable.
Random Forest	Ensemble of many decision trees; handles non-linearity well.
SVM	Separates data with a margin; good for high-dimensional data.
Gradient Boosting	Builds models sequentially, each correcting previous errors; high accuracy.

ğŸ‘‰ In viva:
"I used 4 diverse models â€” simple linear (Logistic), ensemble tree-based (RF, GB), and non-linear boundary (SVM) â€” to compare which performs best."

ğŸ§© STEP 6: PERFORMANCE METRICS

We calculate these for each model:

Accuracy â†’ overall correct predictions.

ROC-AUC â†’ how well it separates churn vs non-churn.

F1 Score â†’ balance between precision & recall.

Precision â†’ how many predicted churns were actually churn.

Recall â†’ how many real churns were correctly caught.

ğŸ‘‰ In viva:
"I used Accuracy, ROC-AUC, F1, Precision, and Recall to evaluate models comprehensively, since accuracy alone can be misleading in imbalanced data."

ğŸ§© STEP 7â€“9: VISUALIZATIONS
1. Bar Chart of Model Performance
fig = px.bar(results_df.melt(...))


Displays performance comparison of all models (interactive).

2. ROC Curve
plt.plot(fpr, tpr)


Shows the tradeoff between true and false positives.

3. Confusion Matrix
sns.heatmap(cm, annot=True)


Displays correct vs incorrect predictions.

ğŸ‘‰ In viva:
"These visuals make it easy to explain model strengths and weaknesses. ROC shows discrimination power, and confusion matrix shows detailed classification."

ğŸ§© STEP 10: FEATURE IMPORTANCE
feat_imp = pd.DataFrame({
    'Feature': X.columns,
    'Importance': best_model.feature_importances_
})


Shows which features influenced churn most (like Contract type, Tenure, Internet service, etc.).

ğŸ‘‰ In viva:
"Feature importance helps interpret which factors drive churn, like month-to-month contracts or high monthly charges."

ğŸ§© STEP 11: SHAP EXPLAINABILITY
explainer = shap.Explainer(best_model, X_train)
shap_values = explainer(X_test)
shap.summary_plot(shap_values, X_test)


SHAP = SHapley Additive exPlanations.

It explains why a model predicted a certain way â€” feature-level influence.

ğŸ‘‰ In viva:
"SHAP provides transparency â€” it shows which features increased or decreased churn probability for each prediction."

ğŸ§© STEP 12â€“15: PREDICTIONS
Predict for all customers:
final_predictions = pd.DataFrame({
    'CustomerID': customer_ids,
    'Actual_Churn': y_all,
    'Predicted_Churn': y_pred_all,
    'Churn_Probability': y_prob_all
})


Combines actual results, predictions, and probability for each customer.

Visualize high-risk customers:
px.histogram(final_predictions, x="Churn_Probability", color="Prediction_Label")


Shows how many customers have high churn risk.

Predict for a new customer:
pred = best_model.predict(new_customer_scaled)[0]
prob = best_model.predict_proba(new_customer_scaled)[0][1]


Predicts whether this new unseen customer is likely to churn and the probability.

ğŸ‘‰ In viva:
"Finally, I used the trained model to predict churn probability for each customer.
I can even input a new customerâ€™s details and the model predicts whether theyâ€™ll leave or stay."

ğŸ’¬ SUMMARY (How to Present to Invigilator)

â€œThis project predicts telecom customer churn using machine learning.
I began with data cleaning and preprocessing (handling missing values, encoding categorical data, scaling).
Then I trained four different models â€” Logistic Regression, Random Forest, SVM, and Gradient Boosting.
I compared their performance using metrics like accuracy, ROC-AUC, and F1-score.
Gradient Boosting gave the best results.
I visualized performance using bar charts, ROC curves, and confusion matrices.
Finally, I used the best model to predict churn probabilities for all customers and visualize high-risk customers.
I also used SHAP explainability to show which features influence churn predictions.â€

A Confusion Matrix is a table that compares:

The actual (true) values from your dataset

Against the predicted values made by your model

It tells you how many were correctly and incorrectly predicted â€” not just overall accuracy, but the types of mistakes your model makes.

ğŸ§± Structure of a Confusion Matrix (for Binary Classification)
	Predicted: No Churn (0)	Predicted: Churn (1)
Actual: No Churn (0)	âœ… True Negative (TN) â€“ model correctly says â€œNo Churnâ€	âŒ False Positive (FP) â€“ model wrongly predicts churn
Actual: Churn (1)	âŒ False Negative (FN) â€“ model misses actual churner	âœ… True Positive (TP) â€“ model correctly predicts churn

